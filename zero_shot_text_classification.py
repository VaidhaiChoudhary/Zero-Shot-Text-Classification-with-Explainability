# -*- coding: utf-8 -*-
"""Zero-Shot Text Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oeHqJ3Q9SFx78lBUsGQqagjSLYa42BWB

# Train Nothing, Label Everything: Zero-Shot Text Classification with Explainability

In a world full of unstructured textâ€”tweets, support tickets, research abstracts, news articlesâ€”automated text classification is both essential and challenging. Traditionally, models required large amounts of labeled data and extensive retraining for each new task.

**Zero-shot learning** offers a smarter solution. Instead of training on labeled examples, we simply define what weâ€™re looking for in plain languageâ€”and let the model infer the rest. With the power of modern **pretrained transformer models**, this becomes not only possible but surprisingly effective.

In this notebook, you'll explore how to:
- Perform **zero-shot classification** using [`facebook/bart-large-mnli`](https://huggingface.co/facebook/bart-large-mnli) via the **Hugging Face ðŸ¤— Transformers** library.
- Accept flexible user-defined labels and score them by **confidence**.
- Visualize top predictions with lightweight **matplotlib** charts.
- Generate natural-language **explanations** for predictions using an open-source **LLM**: [`tiiuae/falcon-rw-1b`](https://huggingface.co/tiiuae/falcon-rw-1b).

No fine-tuning. No training loops. Just smart generalization from models that already understand language.

## Install and Import Required Libraries

Weâ€™ll be using the following:

- **Transformers** by [ðŸ¤— Hugging Face](https://huggingface.co/docs/transformers): A powerful library for using pretrained language models like `facebook/bart-large-mnli`.
- **Torch (PyTorch)**: The underlying framework used for running transformer models.
- **Accelerate** (optional): Helps run models on CPU/GPU efficiently.
- **IPywidgets** (optional): For adding interactivity later, like dropdowns or sliders.

Weâ€™ll also check if GPU is available to speed up inference.

ðŸ“š Refer:
- ðŸ¤— Transformers: https://huggingface.co/docs/transformers
- PyTorch: https://pytorch.org/
"""

# Install required packages
!pip install -q transformers accelerate ipywidgets

# Importing required modules
from transformers import pipeline
import torch

# Check device
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

"""## Load the Zero-Shot Classification Pipeline

Weâ€™ll now load a **zero-shot text classification pipeline** from Hugging Face Transformers.

### Why `facebook/bart-large-mnli`?
Weâ€™re using the model `facebook/bart-large-mnli`, which is pre-trained on the **Multi-Genre Natural Language Inference (MNLI)** task. It can reason whether a given **hypothesis** (label) is *entailed* by a given **premise** (text).

This makes it ideal for **zero-shot classification**, where we treat:
- **Text** â†’ as the *premise*
- **Candidate labels** â†’ as *hypotheses*

ðŸ”— Refer:
- BART Model: https://huggingface.co/facebook/bart-large-mnli
- Zero-shot pipeline docs: https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.ZeroShotClassificationPipeline

"""

# Load zero-shot classification pipeline
classifier = pipeline("zero-shot-classification",
                      model="facebook/bart-large-mnli",
                      device=0 if device == "cuda" else -1)

print("Model loaded successfully!")

"""## Run Basic Zero-Shot Text Classification

Now letâ€™s test the model using a **custom input sentence** and a few **user-defined labels**.

You simply provide:
- A text input (e.g., a news headline or product review)
- A list of candidate labels (e.g., `"sports"`, `"politics"`, `"entertainment"`)

The model will then predict **how likely each label is** for the given text â€” without ever having seen those labels during training!

Letâ€™s see it in action.

"""

# Sample text and labels
text = "The stock market saw record gains this week as tech shares surged."
candidate_labels = ["finance", "technology", "sports", "entertainment"]

# Run zero-shot classification
result = classifier(text, candidate_labels)

# Display results
for label, score in zip(result['labels'], result['scores']):
    print(f"{label:<15} --> Confidence: {score:.4f}")

"""### Visualize Prediction Confidence

To make the model's predictions easier to interpret, weâ€™ll generate a simple horizontal **bar chart** that shows the **confidence scores for the top 3 predicted labels**.

This is helpful in understanding how confidently the model chooses one label over others, especially when predictions are close.

"""

import matplotlib.pyplot as plt

# Multiple test texts for batch classification
texts = [
    "NASA is planning its next mission to Mars.",
    "The new iPhone features an advanced AI chip.",
    "Lionel Messi scored a stunning goal in last nightâ€™s match.",
    "Cryptocurrency prices continue to fall in global markets."
]

candidate_labels = ["sports", "space", "technology", "finance"]

# Classify all texts
batch_results = classifier(texts, candidate_labels)

# Visualize top 3 predictions for each input
for i, result in enumerate(batch_results):
    text = texts[i]
    labels_scores = list(zip(result['labels'], result['scores']))
    top3 = labels_scores[:3]

    print(f"\n{i+1}. \"{text}\"")
    for label, score in top3:
        print(f"   â†’ {label:15} {score:.2f}")

    # Bar chart for top 3 scores
    labels, scores = zip(*top3)
    plt.figure(figsize=(6, 1.5))
    plt.barh(labels, scores, color='skyblue')
    plt.gca().invert_yaxis()
    plt.title(f"Top 3 Predictions for Text {i+1}")
    plt.xlabel("Confidence Score")
    plt.xlim(0, 1)
    plt.tight_layout()
    plt.show()

"""## Try It Yourself â€” Classify Your Own Text!

Want to see how well zero-shot classification works on your own examples?  
In this interactive section, you can input:

- Any **custom sentence or paragraph**
- A list of **labels (topics/categories)** you'd like the model to consider

The model will return which label it thinks fits best â€” along with a confidence score.

This makes it easy to prototype your own tagging or classification logic without training any models!

"""

# Input: User-provided text and labels
text_input = input("Enter the text to classify:\n")
labels_input = input("Enter comma-separated candidate labels (e.g., tech, sports, finance):\n")

# Process label list
candidate_labels = [label.strip() for label in labels_input.split(",") if label.strip()]

# Classify using zero-shot model
result = classifier(text_input, candidate_labels)

# Show predictions
print(f"\nText: \"{text_input}\"")
for label, score in zip(result['labels'], result['scores']):
    print(f"â†’ {label:15} {score:.2f}")

"""The model understands the context of "drug trial" and "Alzheimer's disease" and maps it correctly to the health category â€” without any prior training on your labels.

"Technology" gets the second-highest score, which is reasonable because medical research often overlaps with tech.

Labels like finance, sports, and politics are rightly ranked much lower.

## Label Explanation with a LLM

In this step, we'll generate a natural-language explanation for **why** a predicted label fits a given input text. Weâ€™ll use a free instruction-following model from Hugging Face.

We'll use the [`tiiuae/falcon-rw-1b`](https://huggingface.co/tiiuae/falcon-rw-1b) model â€” a lightweight open-source language model capable of basic reasoning and instruction following.

We'll form a prompt like:
> **"Explain why this text might belong to the label: 'health'..."**

Then generate a short response using Hugging Face's `pipeline()` interface.
"""

from transformers import pipeline

# Load a free instruction-following LLM
generator = pipeline("text-generation", model="tiiuae/falcon-rw-1b")

# Define text and label
text = "A new drug trial shows promising results in treating early-stage Alzheimer's disease."
label = "health"

# Craft prompt
prompt = f"Explain in 1-2 sentences why this text might belong to the label: '{label}'.\n\nText: {text}\nExplanation:"

# Generate explanation
response = generator(prompt, max_new_tokens=60, do_sample=True, temperature=0.7)
print(response[0]['generated_text'].replace(prompt, "").strip())

"""## Conclusion: Smarter Classification, Less Effort

Zero-shot text classification represents a fundamental shift in how we approach language tasks. Instead of hand-labeling data or retraining models for every new use case, we can now **leverage general-purpose language models** to classify text on the flyâ€”using just natural language labels.

In this notebook, you saw how to:
- Use the **`facebook/bart-large-mnli`** transformer model to classify any input text against arbitrary label sets.
- Visualize confidence scores and predictions to better understand the modelâ€™s output.
- Enhance transparency by generating **explanations** using a lightweight **open-source LLM**.

This approach is highly adaptable, scalable, and ideal for rapid prototyping or handling dynamic, evolving taxonomies.

As language models continue to improve, zero-shot techniques like these will become foundational tools for modern NLP workflows.

Want to go further?
- Try with multilingual text or domain-specific data.
- Use richer label descriptions or multi-label settings.
- Integrate with tools like [Label Studio](https://labelstud.io/) for human-in-the-loop refinement.

**Train nothing. Label everything. Thatâ€™s the future.**

"""

